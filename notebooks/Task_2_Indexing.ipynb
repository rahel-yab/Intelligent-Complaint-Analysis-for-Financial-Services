{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d50e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data from Task 1\n",
    "df_cleaned = pd.read_csv('data/processed/filtered_complaints.csv')\n",
    "\n",
    "# Create a stratified sample of 15,000 complaints\n",
    "sample_size = 15000\n",
    "df_sample = df_cleaned.groupby('product_category', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=int(len(x)/len(df_cleaned) * sample_size), random_state=42)\n",
    ")\n",
    "\n",
    "print(\"Sample distribution per product:\")\n",
    "print(df_sample['product_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a2b30",
   "metadata": {},
   "source": [
    "Text Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bcc296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# We use 500 characters with 50 character overlap\n",
    "# Overlap ensures that if a sentence is cut in half, the context is preserved in both chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for _, row in df_sample.iterrows():\n",
    "    # Split the individual complaint\n",
    "    texts = text_splitter.split_text(row['cleaned_narrative'])\n",
    "    \n",
    "    # Create Document objects with metadata for tracing\n",
    "    for i, text in enumerate(texts):\n",
    "        chunks.append(Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"complaint_id\": row.get('Complaint ID', 'N/A'),\n",
    "                \"product\": row['product_category'],\n",
    "                \"issue\": row.get('Issue', 'N/A'),\n",
    "                \"chunk_index\": i\n",
    "            }\n",
    "        ))\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(df_sample)} complaints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636461c6",
   "metadata": {},
   "source": [
    "Embedding and Vector Store (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f3bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize the embedding model\n",
    "# all-MiniLM-L6-v2 is fast, lightweight (80MB), and perfect for 384-dimension vectors\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create and Persist the Vector Store\n",
    "vector_db_path = \"vector_store/chroma_db\"\n",
    "\n",
    "print(\"Indexing chunks... this may take 5-10 minutes depending on your CPU.\")\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=vector_db_path\n",
    ")\n",
    "\n",
    "print(f\"Vector store saved to {vector_db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248a3d2",
   "metadata": {},
   "source": [
    "Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I am having trouble with unauthorized charges on my credit card\"\n",
    "docs = vector_db.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"--- Result {i+1} (Product: {doc.metadata['product']}) ---\")\n",
    "    print(f\"{doc.page_content[:200]}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
